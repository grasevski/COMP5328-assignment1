\documentclass{article} % For LaTeX2e
\usepackage{report,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsfonts}
\graphicspath{{../figures/}}
\addbibresource{references.bib}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Assignment 1}
\author{
  John Hu (zehu4485, 500395897)\\
  Implemented NMF, $L_1$-NMF, $L_{2,1}$-NMF\\
  Wrote most of report
  \and
  Nicholas Grasevski (ngra5777, 500710654)\\
  Implemented KL-NMF, tanh-NMF, CIM-NMF\\
  Wrote experiment part of report
  \and
  Tutor: Yu Yao
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
In this report, we will explore the different NMF algorithms, including traditional NMF, $L_1$-NMF, $L_{2,1}$-NMF, KL-NMF, CIM-NMF and tanh-NMF. Through experimenting on ORL dataset and CroppedYaleB dataset with arbitrary induced noises, we found KL-NMF yields the best results on a clean dataset, whilst CIM-NMF is the most robust out of all algorithms.
\end{abstract}

\section{Introduction}
The Non-negative Matrix Factorization (NMF) is a powerful tool for analysing high dimensional data, e.g. images, movie ratings, stock market values. NMF can be said to be an approximation algorithm, where a higher dimensional matrix $V$ is approximated by two lower rank matrixes $W,H$ such that $V \approx WH$. \cite{wang-zhang} Here the matrix $W$ can be also considered as the weight of matrix $H$, hence the individual component of $V$ is the weight sum of $H$, demonstrated as following:

\begin{equation}
V_i = \begin{bmatrix}
w_{i,1} & w_{i,2} & \dots & w_{i,k-1} & w_{i,k}
\end{bmatrix} \begin{bmatrix}
h_1\\
h_2\\
\dots\\
h_{k-1}\\
h_k
\end{bmatrix}
\end{equation}

In this report, comparison different types of NMF algorithm will be performed. All of the algorithms will be used on two image datasets, and the result in terms of efficiency and accuracy of the algorithms will be shown and discussed.

\section{Related work}
\subsection{Basic NMF and Multiplicative Update Rule (MUR)}
The traditional or basic NMF has an objective function of $min_{W \leq 0, H \leq 0}\left\|V - WH\right\|_F^2$. \cite{lee-seung} It was evident that the basic NMF's objective function is $L_2$-norm; it means the NMF can be sensitive to noise. The multiplicative update rule uses the fact that the Euclidean distance $\left\|V - WH\right\|$ is non-increasing under the rule:

\begin{equation}
H_{kn} \gets H_{kn} \frac{\left(W^T V\right)_{kn}}{\left(W^T WH\right)_{kn}}
\end{equation}

\begin{equation}
W_{mk} \gets W_{mk} \frac{\left(V H^T\right)_{mk}}{\left(WH H^T\right)_{mk}}
\end{equation}

Note that the Euclidean distance is invariant under these updates if and only if $W$ and $H$ are at a stationary point. \cite{marques-maciel-naviner-cai-yang} The simplicity, elegance and computational tractability of the multiplicative update rule made it a popular choice for computing NMFs, even extending its use to semi-nonnegative matrix factorization where $H$ is non-negative, but $X$ can be both negative and non-negative. \cite{ding-li-jordan}

\subsection{L1 and L2 regularized NMF}
The NMF uses a few crucial components of the data to construct an approximation, which means that NMF produces a sparse representation. \cite{luo-peng-fan}

This is a handy feature of the NMF algorithm as most data are sparse in the transform domain, e.g. image in curvelet domain, frequency in Fourier transformation. The $L_1$ and $L_2$ regularized NMF tries to improve the basic NMF by implementing sparse coding and avoiding overfitting. The $L_p$ norm can be shown as below:

\begin{equation}
\left\|a\right\|_p = \left(\sum_{j=1}^{k}\left|a\right|^p\right)^{\frac{1}{p}}
\end{equation}

Hence the objective function of $L_1$ and $L_2$ regularized NMF is calculated by adding the sparsity, in this case the $L_1$ and $L_2$ norm:

\begin{equation}
min_{W \leq 0, H \leq 0}\left\|V - WH\right\|_F^2 + \lambda\Psi\left(R\right)
\end{equation}

where $\Psi\left(R\right)$ is the sparsity constraint.

However, $L_1$ and $L_2$ norm based NMF can be sensitive to outliers and sometimes unstable. The regularization could improve stability but cannot eliminate the impact of outliers.

\subsection{Correntropy Induced Matrix (CIM) NMF}
Instead of the traditional Euclidean distance, CIM is utilising correntropy to calculate the similarity between two variables \cite{liu-pokharel-principe}:

\begin{equation}
\hat{V_\sigma}\left(x,y\right) = \frac{1}{n}\sum_{i=1}^{n}k_\sigma\left(x_i - y_i\right)
\end{equation}

where $k\left(\right)$ is the kernel function, and Gaussian kernel is used. Hence the objective function is:

\begin{equation}
\sum_{i=1}^{m}\sum_{j=1}^{n} 1 - \frac{1}{\sqrt{2\pi}\sigma}\exp\left(\frac{-\left(V - WH\right)_{ij}^2}{2\sigma^2}\right)
\end{equation}

The mathematical property of Gaussian distribution enables the NMF to retain some robustness, but normalization should be considered when implementing this method to ensure the optimization is numerically stable. \cite{du-li-shen}

\subsection{Kullback-Leibler(KL)-NMF}
The Kullback-Leibler divergence is another way that the similarity between two variables can be calculated. \cite{fevotte-idier} In the famous Lee and Seungâ€™s NMF paper, a generalised KL divergence or IS-divergence is used. In this paper, we implement based on the KL-divergence. The distance or the similarity can be calculated as follows:

\begin{equation}
D_{\textup{KL}}\left(V|WH\right) = \sum_{ij}V_{ij} \log\left(\frac{V_{ij}}{\left(WH\right)_{ij}}\right) - V_{ij} + \left(WH\right)_{ij}
\end{equation}

Hence the minimization of this function is the objective. This method could potentially improve robustness compared to traditional NMF. \cite{finesso-spreij}

\subsection{Half-Quadratic minimization}
To compute and find the minimization of the some of the non-quadratic loss functions, the half quadratic minimization technique is used, more recently the half-quadratic programming algorithm is widely used. \cite{geman-chengda} Half quadratic minimization uses half-quadratic theory \cite{nikolova-chan} whereby adding an auxiliary variable can effectively minimize a non-quadratic or non-convex loss function. The technique can be applied to NMF, where the minimization can be solved by fixing the loss function to solve for the auxiliary variable, then use the auxiliary variable to update the loss function, then repeat this process until it converges. \cite{du-li-shen}

\subsection{Tanh NMF}
Tanh NMF is a relatively new NMF algorithm which tries to build a robust NMF by utilising the property of the hyperbolic tangent function. \cite{shen-zhang-lan-liao-luo} To use the tanh function for NMF purpose, Shen has parameterised the tanh function:

\begin{equation}
\tanh\left(ax^2\right) = \frac{e^{ax^2} - e^{-ax^2}}{e^{ax^2} + e^{-ax^2}}
\end{equation}

Hence the similarity function can be written as:

\begin{equation}
\sum_{ij}\tanh\left(aE_{ij}^2\right)
\end{equation}

where

\begin{equation}
E_{ij} = V_{ij} - \sum_r^k W_{ir}H_{rj}
\end{equation}

Then by using the HQ algorithm:

\begin{equation}
\sum_{ij}\left[U_{ij}E_{ij}^2 + \Psi\left(U_{ij}\right)\right]
\end{equation}

where $U_{ij}$ is the auxiliary matrix, and $\Psi\left(U_{ij}\right)$ is the conjugate function of the loss function. It is worth noting that when $E$ (the error term) is fixed, $U$ can be proved to be the following through convex property and Fenchel-Moreau theorem \cite{shen-zhang-lan-liao-luo}:

\begin{equation}
U_{ij} = a\left[1 - \tanh^2\left(a\left|E_{ij}\right|\right)\right]
\end{equation}

This algorithm is a robust and novel method to perform NMF; however, the accuracy still needs improvement as we will show in later sections.

\section{Methods}
\subsection{Pre-processing}
We apply the following preprocessing to the images:
\begin{enumerate}
\item The image size is scaled down to reduce computation time
\item The image data is scaled down from the range $\left[0,255\right]$ to $\left[0,1\right]$ (by dividing by 255), in order to ensure numerical stability and speed up the convergence
\item After applying noise, the image data is clipped to the range $\left[10^{-7},1\right]$ to avoid division by zero in the multiplicative updates as well as ignoring impossible values (darker than pitch black, brighter than maximum brightness)
\end{enumerate}

\subsection{Algorithms and objective functions}
\begin{table}
\begin{tabular}{|c|c|c|}\hline
\textbf{Algorithm} & \textbf{Objective function} & $w\left(X,W,H\right)$\\\hline\hline
NMF & $\left\|V - WH\right\|^2_F$ & $1$\\\hline
KL-NMF & $\sum_{ij} V_{ij}\log\left(\frac{V_{ij}}{\left(WH\right)_{ij}}\right) - V_{ij} + \left(WH\right)_{ij}$ & $1$\\\hline
$L_1$-NMF & $\left\|V - WH\right\|_1$ & $\frac{1}{\left\|V - WH\right\|_1}$\\\hline
$L_{2,1}$-NMF & $\left\|V - WH\right\|_{2,1}$ & $\frac{1}{\left\|V - WH\right\|_{2,1}}$\\\hline
CIM-NMF & $\sum_{i=1}^{m}\sum_{j=1}^{n} 1 - \frac{1}{\sqrt{2\pi}\sigma}e^\frac{-\left(V - WH\right)_{ij}^2}{2\sigma^2}$ & \shortstack{$\exp\left(\frac{-E^2}{2\sigma^2}\right)$\\where $\sigma^2 = \frac{1}{2mn}\sum_{i=1}^{m}\sum_{j=1}^{n}E_{ij}^2$}\\\hline
tanh-NMF & \shortstack{$\sum_{ij}\tanh\left(aE_{ij}^2\right)$\\ where $a = \frac{nmp}{\sum_{i=1}{m}\sum_{j=1}{n}E_{ij}^2}$\\ and $p \in \left\{1\right\}$} & \shortstack{$a\left[1 - \tanh^2\left(a\left|E\right|\right)\right]$\\ where $a = \frac{nmp}{\sum_{i=1}^{m}\sum_{j=1}^{n}E_{ij}^2}$\\ and $p \in \left\{1\right\}$}\\\hline
\end{tabular}
\caption{Algorithms and objective functions, where $E = V - WH$ \label{tab:objfunc}}
\end{table}

\begin{algorithm}
\begin{algorithmic}
\REQUIRE $V \in \mathbb{R}^{m \times n}, k \in \mathbb{Z}^+, \beta,\lambda_{W1},\lambda_{W2},\lambda_{H1},\lambda_{H2} \in \mathbb{R},w \in \left(\mathbb{R}^{m \times n}, \mathbb{R}^{m \times k}, \mathbb{R}^{k \times n}\right) \to \mathbb{R}^{m \times n}$
\ENSURE $W \in \mathbb{R}^{m \times k}, H \in \mathbb{R}^{k \times n}$
\STATE $W,H \gets U\left(0,1\right),U\left(0,1\right)$
\REPEAT
  \STATE $U \gets w\left(X,W,H\right)$
  \STATE $W \gets W \otimes \frac{\left[U \otimes V \otimes \left(WH\right)^{\beta-2}\right] H^T}{\left[U \otimes \left(WH\right)^{\beta-1}\right]H^T + \lambda_{W1} + \lambda_{W2} W}$
  \STATE $H \gets H \otimes \frac{W^T \left[U \otimes V \otimes \left(WH\right)^{\beta-2}\right]}{W^T \left[U \otimes \left(WH\right)^{\beta-1}\right] + \lambda_{H1} + \lambda_{H2} H}$
\UNTIL{$\textup{converged}$}
\end{algorithmic}
\caption{Weighted NMF with $\beta$-divergence and regularization\label{alg:b-wnmf}}
\end{algorithm}

All the NMF algorithms used and their objective functions are shown in table \ref{tab:objfunc}. All algorithms use the weighted multiplicative update rule to compute $W$ and $H$. Therefore iteratively, all algorithms can be implemented by adjusting weight according to its loss function. \cite{arora} Algorithm \ref{alg:b-wnmf} illustrates the generic procedure used by all of the NMF variants tested in this experiment.

Please note $\beta,\lambda_{W1},\lambda_{W2},\lambda_{H1},\lambda_{H2}$ allows a broader range of extensions to the algorithm (different objective functions, different regularization) and combinations thereof than have been explored in this experiment. In this experiment, we set $\beta=1$ for KL-NMF, and $\beta=2$ for algorithms. No regularization is used, i.e. $\lambda_{W1}=\lambda_{W2}=\lambda_{H1}=\lambda_{H2}=0$.

Before experimenting, the objective function shows that theoretically CIM and Tanh NMFs are more robust than other methods, as both Gaussian kernel and tanh function are not sensitive to outliers. It is due to comparing gaussian and tanh to a linear or quadratic function. Extreme values hava a relatively lesser impact when using gaussian or tanh function to update the weight matrix.

Each algorithm is applied to both datasets, and evaluation metrics are recorded.

\subsection{Introduced noises}
Noises are introduced into the data, and algorithms are evaluated again on the noisy data. The result of noisy data will be compared vertically and horizontally. It will allow us to understand the robustness of the algorithm.

\subsubsection{Salt and pepper noise}
\begin{figure}
\includegraphics[width=\linewidth]{ORL-salt_and_pepper-3.png}
\caption{ORL salt and pepper noise, $\sigma=0.4$ \label{fig:ORL-salt_and_pepper-3}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-salt_and_pepper-3.png}
\caption{CroppedYaleB salt and pepper noise, $\sigma=0.4$ \label{fig:CroppedYaleB-salt_and_pepper-3}}
\end{figure}

During an analog-to-digital conversion, an image may obtain some dark pixels in bright regions and bright pixels in dark regions. \cite{noise} Although modern techniques can effectively eliminate this noise, the salt and pepper noise was added to further examine the robustness of the algorithms. The noise was applied like so:

\begin{equation}
V = \hat{V} + 1\left\{P \land R\right\} - 1\left\{P \land \lnot R\right\}
\end{equation}

where $V$ is the noisy image, $\hat{V}$ is the original image, $P \equiv z_1 \leq \sigma$, $R \equiv z_2 \leq r$, $z_1 \sim N\left(0,\sigma\right)$, $z_1 \sim N\left(0,r\right)$, $\sigma \in \left\{0.1,0.2,0.3,0.4\right\}$ is the noise level hyperparameter and $r \in \left\{0.3\right\} $ is the white-to-black ratio hyperparameter. Figures \ref{fig:ORL-salt_and_pepper-3} and \ref{fig:CroppedYaleB-salt_and_pepper-3} demonstrate how the various algorithms cope with this noise on the ORL and CroppedYaleB datasets respectively.

\subsubsection{Laplace noise}
\begin{figure}
\includegraphics[width=\linewidth]{ORL-laplace-3.png}
\caption{ORL laplace noise, $\sigma=0.4$ \label{fig:ORL-laplace-3}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-laplace-3.png}
\caption{CroppedYaleB laplace noise, $\sigma=0.4$ \label{fig:CroppedYaleB-laplace-3}}
\end{figure}

Laplace noise refers to the noise in the statistical model used, as well as the noise during the processing of signals, for example, when taking a photo using a digital sensor, the sensor would have inherent noises due to its temperature, level of illumination and electrical wiring. Therefore, a perfect image with no noise is most impossible to obtain in practice, so when processing an image signal using NMF, we need to account for the noises. It is also worth to mention that noises are mostly random in nature. In principle the $L_1$-NMF should handle this type of noise well because the $L_1$ norm assumes a Laplace distribution. \cite{yang-zhang-yang-zhang} The noise was applied like so:

\begin{equation}
V = \hat{V} + Z
\end{equation}

where $V$ is the noisy image, $\hat{V}$ is the original image, $Z \sim \textup{Laplace}\left(0,\sigma\right)$ and $\sigma \in \left\{0.1,0.2,0.3,0.4\right\}$ is the scale hyperparameter. Figures \ref{fig:ORL-laplace-3} and \ref{fig:CroppedYaleB-laplace-3} demonstrate how the various algorithms cope with this noise on the ORL and CroppedYaleB datasets respectively.

\subsubsection{Uniform noise}
\begin{figure}
\includegraphics[width=\linewidth]{ORL-uniform-3.png}
\caption{ORL uniform noise, $\sigma=0.4$ \label{fig:ORL-uniform-3}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-uniform-3.png}
\caption{CroppedYaleB uniform noise, $\sigma=0.4$ \label{fig:CroppedYaleB-uniform-3}}
\end{figure}

Being digital, the pixels of a given image can sometimes be quantized to a number of discrete levels, for example if the original image had a low resolution. This is known as quantization noise \cite{noise} We approximate this effect by adding uniform noise to the original image. The noise was applied like so:

\begin{equation}
V = \hat{V} + Z
\end{equation}

where $V$ is the noisy image, $\hat{V}$ is the original image, $Z \sim U\left(-\sigma,\sigma\right)$ and $\sigma \in \left\{0.1,0.2,0.3,0.4\right\}$ is the noise level hyperparameter. Figures \ref{fig:ORL-uniform-3} and \ref{fig:CroppedYaleB-uniform-3} demonstrates how the various algorithms cope with this noise on the ORL and CroppedYaleB datasets respectively.

\subsubsection{Gaussian noise}
\begin{figure}
\includegraphics[width=\linewidth]{ORL-gaussian-3.png}
\caption{ORL gaussian noise, $\sigma=0.4$ \label{fig:ORL-gaussian-3}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-gaussian-3.png}
\caption{CroppedYaleB gaussian noise, $\sigma=0.4$ \label{fig:CroppedYaleB-gaussian-3}}
\end{figure}

Gaussian noise is noise with a normal distribution. It can arise in digital images due to sensor noise, poor illumination, high temperature or electronic circuit noise. \cite{gaussian} Frobenius norm already assumes gaussian noise, thus standard NMF is well equipped to handle this kind of corruption. Nevertheless it is included for comparison as it is a common type of noise in images. The noise was applied like so:

\begin{equation}
V = \hat{V} + Z
\end{equation}

where $V$ is the noisy image, $\hat{V}$ is the original image, $Z \sim N\left(0,\sigma^2\right)$ and $\sigma \in \left\{0.1,0.2,0.3,0.4\right\}$ is the standard deviation hyperparameter. Figures \ref{fig:ORL-gaussian-3} and \ref{fig:CroppedYaleB-gaussian-3} demonstrate how the various algorithms cope with this noise on the ORL and CroppedYaleB datasets respectively.

\section{Experiment}
The experiment will evaluate the algorithms introduced in method on the ORL and CroppedYaleB datasets which will be explained in detail later. The algorithm will be evaluated in turn on each noise type listed above.

The algorithms are run on 5 different samplings of 90\% of the data and the mean and standard deviation of the results are recorded. Due to the sensitivity of NMF to initialization, each algorithm is given the same (randomly chosen) initialization point in a given trial to ensure a fair comparison.

\subsection{Dataset}
The datasets used in this report are ORL, and CroppedYaleB. ORL is a facial image dataset, it contains 400 images with a size of $\left(112,92\right)$. CroppedYaleB is a dataset cropped from YaleB, YaleB is a very large dataset with 5760 images, done by 10 subjects posing 9 photos under 64 different light conditions.

\subsection{Evaluation metrics}
In order to perform a comprehensive comparison between the algorithms, several evaluation metrics are used: Relative Reconstruction Errors (RRE), Average Accuracy and Normalized Mutual Information (NMI). For the purposes of evaluation, we set the number of components to the number of classes (i.e. people) in the original datasets, so as to be able to meaningfully calculate the metrics described below.

\subsubsection{Relative Reconstruction Errors}
Relative Reconstruction Errors demonstrate how well the algorithm can regenerate the original data with or without added noise. It is calculated using the following formula:

\begin{equation}
\textup{RRE} = \frac{\left\|\hat{V} - WH\right\|_F}{\left\|\hat{V}\right\|_F}
\end{equation}

where $\hat{V}$ is the clean dataseet and $W,H$ are the factorization of $V$.

\subsubsection{Average Accuracy}
The average accuracy is done according to K-means clustering, with K set to the number of classes in the dataset (number of individuals), then based on the clustering the average accuracy can be calculated using the prediction $Y_{\textup{pred}}$ as follows:

\begin{equation}
\textup{Acc}\left(Y,Y_{\textup{pred}}\right) = \frac{1}{n}\sum_{i=1}^{n}1\left\{Y_{\textup{pred}}\left(i\right) = Y\left(i\right)\right\}
\end{equation}

\subsubsection{Normalized Mutual Information}
The normalized mutual information allows us to better understand the average accuracy, as it maximizes the effect of true prediction:

\begin{equation}
\textup{NMI}\left(Y,Y_{\textup{pred}}\right) = \frac{2I\left(Y,Y_{\textup{pred}}\right)}{H\left(Y\right) + H\left(Y_{\textup{pred}}\right)}
\end{equation}

where $I\left(\cdot,\cdot\right)$ is mutual information and $H\left(\cdot\right)$ is entropy.

\subsection{Results}
\begin{figure}
\includegraphics[width=\linewidth]{ORL-salt_and_pepper.png}
\caption{ORL salt and pepper noise level vs evaluation metrics \label{fig:ORL-salt_and_pepper}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-salt_and_pepper.png}
\caption{CroppedYaleB salt and pepper noise level vs evaluation metrics \label{fig:CroppedYaleB-salt_and_pepper}}
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{ORL-laplace.png}
\caption{ORL laplace noise standard deviation vs evaluation metrics \label{fig:ORL-laplace}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-laplace.png}
\caption{CroppedYaleB laplace noise standard deviation vs evaluation metrics \label{fig:CroppedYaleB-laplace}}
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{ORL-uniform.png}
\caption{ORL uniform noise level vs evaluation metrics \label{fig:ORL-uniform}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-uniform.png}
\caption{CroppedYaleB uniform noise level vs evaluation metrics \label{fig:CroppedYaleB-uniform}}
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{ORL-gaussian.png}
\caption{ORL gaussian noise level vs evaluation metrics \label{fig:ORL-gaussian}}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{CroppedYaleB-gaussian.png}
\caption{CroppedYaleB gaussian noise level vs evaluation metrics \label{fig:CroppedYaleB-gaussian}}
\end{figure}

Figures \ref{fig:ORL-salt_and_pepper} and \ref{fig:CroppedYaleB-salt_and_pepper} show tanh-NMF and CIM-NMF performing well as the salt-and-pepper noise is increased. However for the other types of noise, they perform relatively poorly. Perhaps this is unsurprising, given that the other types of noise do not simulate outliers and furthermore they match too favorably to the underlying assumptions (NMF with Gaussian noise, $L_1$-NMF with Laplace noise, etc).


\subsection{Discussion}
From the result, we can observe that NMF, KL-NMF, $L_1$-NMF, and $L_{2,1}$-NMF have similar results, where the impact of noise on the accuracy, NMI and RRE are very much alike. These algorithms are all capable of generating good results without noise; however, they are susceptible to contaminated data because the result has shown that their accuracy decreases significantly when exposed to noise. The result matches the prediction made based on the mathematical property of the objective functions, cim-NMF or tanh-NMF are more robust than the other algorithms. Cim-NMF or tanh-NMF are less sensitive to noise, and they performed relatively well even with noise introduced.

However, there is a trade-off between robustness and accuracy. Although the robust algorithms are relatively insensitive to noise, a lower level of accuracy is shown in comparison to the other algorithms. 

\section{Conclusion}
In this report, we have compared several different NMF algorithms. The best performing algorithm, on a clean dataset is the KL-NMF, the most robust algorithm being CIM-NMF. Based on the results, there is an apparent trade-off between robustness and accuracy. Perhaps the future research of NMF algorithm can focus on improving the accuracy while retaining the robust property of the algorithm.

\printbibliography\appendix

\section{Running the code}
\begin{itemize}
\item Dependencies: \texttt{pip install matplotlib scikit-learn}
\item Usage: \texttt{./code/algorithm/algorithm.py}
\item Help: \texttt{./code/algorithm/algorithm.py -h}
\end{itemize}

\section{Detailed Experiment Results}

\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 19.31 $\pm$ 0.05 & 19.37 $\pm$ 0.04 & 19.31 $\pm$ 0.05 & 19.28 $\pm$ 0.05 & 16.88 $\pm$ 0.13 & 44.84 $\pm$ 0.62 \\
0.2 & 24.99 $\pm$ 0.07 & 25.40 $\pm$ 0.09 & 24.99 $\pm$ 0.07 & 24.99 $\pm$ 0.07 & 16.29 $\pm$ 0.17 & 58.80 $\pm$ 0.33 \\
0.3 & 30.17 $\pm$ 0.12 & 30.99 $\pm$ 0.11 & 30.17 $\pm$ 0.12 & 30.18 $\pm$ 0.12 & 18.33 $\pm$ 0.25 & 66.35 $\pm$ 0.47 \\
0.4 & 35.04 $\pm$ 0.16 & 36.25 $\pm$ 0.16 & 35.04 $\pm$ 0.16 & 35.06 $\pm$ 0.18 & 26.50 $\pm$ 0.36 & 74.83 $\pm$ 0.51 \\
\end{tabular}\caption{
  RRE(\%) on ORL dataset with salt-and-pepper noise (mean $\pm$ std)
  \label{tab:RRE-ORL-salt-and-pepper}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 64.72 $\pm$ 3.49 & 67.00 $\pm$ 1.78 & 64.72 $\pm$ 3.49 & 66.61 $\pm$ 1.72 & 71.06 $\pm$ 1.68 & 53.89 $\pm$ 2.31 \\
0.2 & 47.44 $\pm$ 2.30 & 49.17 $\pm$ 5.15 & 47.44 $\pm$ 2.30 & 48.39 $\pm$ 2.51 & 71.72 $\pm$ 3.14 & 64.50 $\pm$ 3.38 \\
0.3 & 32.89 $\pm$ 0.97 & 32.50 $\pm$ 0.98 & 32.89 $\pm$ 0.97 & 32.72 $\pm$ 1.68 & 68.94 $\pm$ 2.46 & 62.83 $\pm$ 1.91 \\
0.4 & 25.28 $\pm$ 0.91 & 25.50 $\pm$ 1.83 & 25.28 $\pm$ 0.91 & 25.00 $\pm$ 1.18 & 55.94 $\pm$ 3.73 & 38.44 $\pm$ 1.96 \\
\end{tabular}\caption{
  Acc(\%) on ORL dataset with salt-and-pepper noise (mean $\pm$ std)
  \label{tab:Acc-ORL-salt-and-pepper}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 78.66 $\pm$ 2.70 & 81.06 $\pm$ 1.26 & 78.66 $\pm$ 2.70 & 79.74 $\pm$ 1.02 & 84.34 $\pm$ 1.34 & 70.59 $\pm$ 1.79 \\
0.2 & 64.58 $\pm$ 1.72 & 65.42 $\pm$ 4.01 & 64.58 $\pm$ 1.72 & 65.35 $\pm$ 1.43 & 84.24 $\pm$ 1.09 & 80.40 $\pm$ 1.73 \\
0.3 & 51.66 $\pm$ 0.31 & 51.43 $\pm$ 0.69 & 51.66 $\pm$ 0.31 & 52.26 $\pm$ 1.64 & 82.88 $\pm$ 1.88 & 79.30 $\pm$ 1.45 \\
0.4 & 44.30 $\pm$ 1.56 & 44.29 $\pm$ 1.88 & 44.30 $\pm$ 1.56 & 44.36 $\pm$ 1.00 & 72.83 $\pm$ 2.03 & 59.08 $\pm$ 1.38 \\
\end{tabular}\caption{
  NMI(\%) on ORL dataset with salt-and-pepper noise (mean $\pm$ std)
  \label{tab:NMI-ORL-salt-and-pepper}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 15.07 $\pm$ 1.67 & 14.92 $\pm$ 1.59 & 15.07 $\pm$ 1.67 & 15.13 $\pm$ 1.65 & 22.54 $\pm$ 1.33 & 37.25 $\pm$ 1.86 \\
0.2 & 17.76 $\pm$ 4.93 & 17.61 $\pm$ 4.87 & 17.76 $\pm$ 4.93 & 17.82 $\pm$ 4.92 & 24.69 $\pm$ 4.10 & 37.01 $\pm$ 2.68 \\
0.3 & 21.15 $\pm$ 8.61 & 21.01 $\pm$ 8.56 & 21.15 $\pm$ 8.61 & 21.20 $\pm$ 8.59 & 27.53 $\pm$ 7.49 & 37.81 $\pm$ 3.45 \\
0.4 & 24.91 $\pm$ 12.33 & 24.79 $\pm$ 12.29 & 24.91 $\pm$ 12.33 & 24.96 $\pm$ 12.32 & 30.81 $\pm$ 11.12 & 39.41 $\pm$ 5.49 \\
\end{tabular}\caption{
  RRE(\%) on ORL dataset with laplace noise (mean $\pm$ std)
  \label{tab:RRE-ORL-laplace}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 73.00 $\pm$ 1.76 & 75.50 $\pm$ 3.78 & 73.00 $\pm$ 1.76 & 75.28 $\pm$ 3.54 & 61.33 $\pm$ 1.90 & 29.67 $\pm$ 2.13 \\
0.2 & 72.44 $\pm$ 2.02 & 76.11 $\pm$ 1.44 & 72.44 $\pm$ 2.02 & 74.50 $\pm$ 3.35 & 63.00 $\pm$ 2.57 & 28.67 $\pm$ 3.63 \\
0.3 & 73.39 $\pm$ 0.90 & 75.50 $\pm$ 1.41 & 73.39 $\pm$ 0.90 & 76.50 $\pm$ 2.28 & 61.00 $\pm$ 4.60 & 28.33 $\pm$ 3.93 \\
0.4 & 73.06 $\pm$ 1.98 & 74.33 $\pm$ 2.18 & 73.06 $\pm$ 1.98 & 75.56 $\pm$ 3.29 & 60.94 $\pm$ 4.63 & 27.94 $\pm$ 6.52 \\
\end{tabular}\caption{
  Acc(\%) on ORL dataset with laplace noise (mean $\pm$ std)
  \label{tab:Acc-ORL-laplace}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 85.75 $\pm$ 0.80 & 87.02 $\pm$ 1.72 & 85.75 $\pm$ 0.80 & 85.59 $\pm$ 1.84 & 78.34 $\pm$ 1.48 & 47.25 $\pm$ 2.40 \\
0.2 & 84.78 $\pm$ 1.28 & 87.13 $\pm$ 0.98 & 84.78 $\pm$ 1.28 & 85.66 $\pm$ 1.91 & 78.57 $\pm$ 2.13 & 47.35 $\pm$ 2.62 \\
0.3 & 85.32 $\pm$ 0.44 & 86.35 $\pm$ 0.62 & 85.32 $\pm$ 0.44 & 87.53 $\pm$ 1.30 & 77.31 $\pm$ 3.45 & 46.35 $\pm$ 4.24 \\
0.4 & 85.47 $\pm$ 0.62 & 86.21 $\pm$ 1.11 & 85.47 $\pm$ 0.62 & 86.09 $\pm$ 2.08 & 77.36 $\pm$ 3.56 & 45.85 $\pm$ 6.61 \\
\end{tabular}\caption{
  NMI(\%) on ORL dataset with laplace noise (mean $\pm$ std)
  \label{tab:NMI-ORL-laplace}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 13.96 $\pm$ 0.04 & 13.90 $\pm$ 0.05 & 13.96 $\pm$ 0.04 & 14.01 $\pm$ 0.02 & 21.41 $\pm$ 0.07 & 37.97 $\pm$ 0.24 \\
0.2 & 14.35 $\pm$ 0.04 & 14.35 $\pm$ 0.05 & 14.35 $\pm$ 0.04 & 14.39 $\pm$ 0.02 & 21.28 $\pm$ 0.09 & 36.70 $\pm$ 0.25 \\
0.3 & 15.04 $\pm$ 0.05 & 15.13 $\pm$ 0.06 & 15.04 $\pm$ 0.05 & 15.07 $\pm$ 0.03 & 21.96 $\pm$ 0.13 & 35.15 $\pm$ 0.21 \\
0.4 & 16.04 $\pm$ 0.06 & 16.24 $\pm$ 0.08 & 16.04 $\pm$ 0.06 & 16.07 $\pm$ 0.04 & 23.60 $\pm$ 0.11 & 34.20 $\pm$ 0.06 \\
\end{tabular}\caption{
  RRE(\%) on ORL dataset with uniform noise (mean $\pm$ std)
  \label{tab:RRE-ORL-uniform}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 72.83 $\pm$ 4.05 & 76.06 $\pm$ 1.12 & 72.83 $\pm$ 4.05 & 74.00 $\pm$ 3.80 & 61.83 $\pm$ 2.27 & 23.78 $\pm$ 1.52 \\
0.2 & 73.00 $\pm$ 2.92 & 73.44 $\pm$ 2.54 & 73.00 $\pm$ 2.92 & 72.61 $\pm$ 3.42 & 60.50 $\pm$ 2.04 & 21.44 $\pm$ 0.48 \\
0.3 & 73.00 $\pm$ 2.29 & 73.28 $\pm$ 2.35 & 73.00 $\pm$ 2.29 & 73.33 $\pm$ 2.13 & 53.83 $\pm$ 2.20 & 19.11 $\pm$ 0.73 \\
0.4 & 72.78 $\pm$ 2.73 & 71.94 $\pm$ 2.72 & 72.78 $\pm$ 2.73 & 70.50 $\pm$ 2.04 & 44.39 $\pm$ 3.23 & 18.89 $\pm$ 0.53 \\
\end{tabular}\caption{
  Acc(\%) on ORL dataset with uniform noise (mean $\pm$ std)
  \label{tab:Acc-ORL-uniform}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 85.08 $\pm$ 1.97 & 86.82 $\pm$ 0.93 & 85.08 $\pm$ 1.97 & 85.84 $\pm$ 1.69 & 78.80 $\pm$ 0.79 & 42.28 $\pm$ 2.17 \\
0.2 & 85.11 $\pm$ 1.91 & 85.56 $\pm$ 0.70 & 85.11 $\pm$ 1.91 & 85.06 $\pm$ 1.80 & 76.61 $\pm$ 1.69 & 39.46 $\pm$ 0.77 \\
0.3 & 84.90 $\pm$ 1.41 & 85.32 $\pm$ 1.38 & 84.90 $\pm$ 1.41 & 85.01 $\pm$ 1.39 & 70.81 $\pm$ 1.55 & 38.14 $\pm$ 1.33 \\
0.4 & 84.13 $\pm$ 1.27 & 83.75 $\pm$ 1.03 & 84.13 $\pm$ 1.27 & 83.64 $\pm$ 1.32 & 63.26 $\pm$ 1.57 & 37.00 $\pm$ 1.51 \\
\end{tabular}\caption{
  NMI(\%) on ORL dataset with uniform noise (mean $\pm$ std)
  \label{tab:NMI-ORL-uniform}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 34.26 $\pm$ 11.90 & 34.03 $\pm$ 11.93 & 34.26 $\pm$ 11.90 & 34.29 $\pm$ 11.89 & 39.17 $\pm$ 10.63 & 39.18 $\pm$ 4.29 \\
0.2 & 60.56 $\pm$ 24.41 & 60.38 $\pm$ 24.48 & 60.56 $\pm$ 24.41 & 60.58 $\pm$ 24.41 & 64.86 $\pm$ 23.76 & 59.78 $\pm$ 18.95 \\
0.3 & 81.61 $\pm$ 31.78 & 81.49 $\pm$ 31.87 & 81.61 $\pm$ 31.78 & 81.63 $\pm$ 31.78 & 86.19 $\pm$ 31.88 & 80.27 $\pm$ 29.36 \\
0.4 & 94.70 $\pm$ 33.17 & 94.64 $\pm$ 33.28 & 94.70 $\pm$ 33.17 & 94.73 $\pm$ 33.18 & 97.62 $\pm$ 32.15 & 93.44 $\pm$ 32.70 \\
\end{tabular}\caption{
  RRE(\%) on ORL dataset with gaussian noise (mean $\pm$ std)
  \label{tab:RRE-ORL-gaussian}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 73.00 $\pm$ 2.39 & 74.33 $\pm$ 2.51 & 73.00 $\pm$ 2.39 & 74.39 $\pm$ 1.59 & 59.39 $\pm$ 2.92 & 22.56 $\pm$ 2.29 \\
0.2 & 71.78 $\pm$ 1.74 & 74.83 $\pm$ 3.02 & 71.78 $\pm$ 1.74 & 73.11 $\pm$ 1.84 & 48.39 $\pm$ 8.46 & 19.72 $\pm$ 2.73 \\
0.3 & 59.89 $\pm$ 12.59 & 62.22 $\pm$ 13.11 & 59.89 $\pm$ 12.59 & 61.06 $\pm$ 12.75 & 35.94 $\pm$ 19.12 & 19.17 $\pm$ 2.10 \\
0.4 & 42.00 $\pm$ 24.22 & 42.00 $\pm$ 24.31 & 42.00 $\pm$ 24.22 & 40.83 $\pm$ 24.13 & 30.83 $\pm$ 16.48 & 17.89 $\pm$ 1.67 \\
\end{tabular}\caption{
  Acc(\%) on ORL dataset with gaussian noise (mean $\pm$ std)
  \label{tab:Acc-ORL-gaussian}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 85.50 $\pm$ 1.33 & 85.59 $\pm$ 1.06 & 85.50 $\pm$ 1.33 & 85.65 $\pm$ 1.11 & 75.76 $\pm$ 1.55 & 41.33 $\pm$ 3.34 \\
0.2 & 84.56 $\pm$ 0.84 & 86.30 $\pm$ 1.85 & 84.56 $\pm$ 0.84 & 84.95 $\pm$ 0.92 & 66.70 $\pm$ 7.21 & 37.15 $\pm$ 3.43 \\
0.3 & 75.32 $\pm$ 9.81 & 77.36 $\pm$ 10.60 & 75.32 $\pm$ 9.81 & 75.99 $\pm$ 10.05 & 53.08 $\pm$ 17.38 & 37.54 $\pm$ 1.69 \\
0.4 & 58.32 $\pm$ 21.58 & 58.10 $\pm$ 22.34 & 58.32 $\pm$ 21.58 & 56.90 $\pm$ 22.04 & 47.68 $\pm$ 17.00 & 35.47 $\pm$ 2.14 \\
\end{tabular}\caption{
  NMI(\%) on ORL dataset with gaussian noise (mean $\pm$ std)
  \label{tab:NMI-ORL-gaussian}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 21.88 $\pm$ 0.07 & 22.62 $\pm$ 0.07 & 21.88 $\pm$ 0.07 & 21.90 $\pm$ 0.08 & 24.54 $\pm$ 0.23 & 57.43 $\pm$ 0.36 \\
0.2 & 26.22 $\pm$ 0.04 & 27.33 $\pm$ 0.07 & 26.22 $\pm$ 0.04 & 26.25 $\pm$ 0.04 & 24.67 $\pm$ 0.20 & 66.75 $\pm$ 0.40 \\
0.3 & 31.82 $\pm$ 0.08 & 32.79 $\pm$ 0.10 & 31.82 $\pm$ 0.08 & 31.84 $\pm$ 0.08 & 27.40 $\pm$ 0.25 & 74.57 $\pm$ 0.15 \\
0.4 & 37.88 $\pm$ 0.10 & 38.53 $\pm$ 0.12 & 37.88 $\pm$ 0.10 & 37.91 $\pm$ 0.11 & 34.20 $\pm$ 0.38 & 81.09 $\pm$ 0.16 \\
\end{tabular}\caption{
  RRE(\%) on CroppedYaleB dataset with salt-and-pepper noise (mean $\pm$ std)
  \label{tab:RRE-CroppedYaleB-salt-and-pepper}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 21.97 $\pm$ 0.99 & 20.95 $\pm$ 1.03 & 21.97 $\pm$ 0.99 & 21.08 $\pm$ 1.51 & 19.35 $\pm$ 0.51 & 10.59 $\pm$ 0.18 \\
0.2 & 18.90 $\pm$ 1.21 & 17.28 $\pm$ 1.30 & 18.90 $\pm$ 1.21 & 19.52 $\pm$ 1.05 & 20.44 $\pm$ 0.63 & 13.02 $\pm$ 0.52 \\
0.3 & 16.68 $\pm$ 0.77 & 14.44 $\pm$ 0.78 & 16.68 $\pm$ 0.77 & 16.22 $\pm$ 0.79 & 20.35 $\pm$ 0.61 & 14.93 $\pm$ 0.71 \\
0.4 & 12.80 $\pm$ 0.55 & 12.09 $\pm$ 0.32 & 12.80 $\pm$ 0.55 & 13.15 $\pm$ 0.56 & 18.77 $\pm$ 0.46 & 12.54 $\pm$ 0.90 \\
\end{tabular}\caption{
  Acc(\%) on CroppedYaleB dataset with salt-and-pepper noise (mean $\pm$ std)
  \label{tab:Acc-CroppedYaleB-salt-and-pepper}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 29.25 $\pm$ 1.14 & 29.10 $\pm$ 1.41 & 29.25 $\pm$ 1.14 & 28.93 $\pm$ 1.59 & 23.63 $\pm$ 1.92 & 12.40 $\pm$ 0.79 \\
0.2 & 26.07 $\pm$ 0.96 & 24.84 $\pm$ 1.37 & 26.07 $\pm$ 0.96 & 25.69 $\pm$ 0.89 & 25.67 $\pm$ 0.75 & 16.69 $\pm$ 1.00 \\
0.3 & 21.81 $\pm$ 0.67 & 19.72 $\pm$ 1.62 & 21.81 $\pm$ 0.67 & 20.95 $\pm$ 0.67 & 26.72 $\pm$ 1.08 & 18.41 $\pm$ 0.51 \\
0.4 & 16.03 $\pm$ 0.70 & 14.66 $\pm$ 0.90 & 16.03 $\pm$ 0.70 & 16.08 $\pm$ 0.50 & 24.06 $\pm$ 0.92 & 15.15 $\pm$ 0.88 \\
\end{tabular}\caption{
  NMI(\%) on CroppedYaleB dataset with salt-and-pepper noise (mean $\pm$ std)
  \label{tab:NMI-CroppedYaleB-salt-and-pepper}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 20.89 $\pm$ 2.00 & 21.12 $\pm$ 1.76 & 20.89 $\pm$ 2.00 & 21.08 $\pm$ 1.97 & 29.41 $\pm$ 1.26 & 52.33 $\pm$ 0.55 \\
0.2 & 24.18 $\pm$ 6.16 & 24.37 $\pm$ 5.88 & 24.18 $\pm$ 6.16 & 24.35 $\pm$ 6.12 & 31.82 $\pm$ 4.48 & 53.20 $\pm$ 1.39 \\
0.3 & 28.32 $\pm$ 10.93 & 28.48 $\pm$ 10.69 & 28.32 $\pm$ 10.93 & 28.48 $\pm$ 10.90 & 35.20 $\pm$ 8.73 & 54.99 $\pm$ 3.43 \\
0.4 & 32.94 $\pm$ 15.83 & 33.07 $\pm$ 15.62 & 32.94 $\pm$ 15.83 & 33.07 $\pm$ 15.80 & 39.16 $\pm$ 13.37 & 57.26 $\pm$ 6.27 \\
\end{tabular}\caption{
  RRE(\%) on CroppedYaleB dataset with laplace noise (mean $\pm$ std)
  \label{tab:RRE-CroppedYaleB-laplace}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 23.32 $\pm$ 0.64 & 23.85 $\pm$ 0.97 & 23.32 $\pm$ 0.64 & 24.07 $\pm$ 1.74 & 13.93 $\pm$ 0.69 & 8.56 $\pm$ 0.30 \\
0.2 & 23.21 $\pm$ 0.76 & 24.48 $\pm$ 1.20 & 23.21 $\pm$ 0.76 & 23.40 $\pm$ 1.03 & 13.95 $\pm$ 1.06 & 8.69 $\pm$ 0.17 \\
0.3 & 23.52 $\pm$ 0.83 & 23.77 $\pm$ 1.23 & 23.52 $\pm$ 0.83 & 22.54 $\pm$ 0.73 & 13.44 $\pm$ 1.44 & 8.62 $\pm$ 0.15 \\
0.4 & 22.04 $\pm$ 1.80 & 23.96 $\pm$ 1.29 & 22.04 $\pm$ 1.80 & 22.85 $\pm$ 1.34 & 13.16 $\pm$ 1.49 & 8.40 $\pm$ 0.31 \\
\end{tabular}\caption{
  Acc(\%) on CroppedYaleB dataset with laplace noise (mean $\pm$ std)
  \label{tab:Acc-CroppedYaleB-laplace}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 31.57 $\pm$ 0.83 & 31.39 $\pm$ 1.61 & 31.57 $\pm$ 0.83 & 31.84 $\pm$ 1.56 & 16.67 $\pm$ 1.00 & 8.93 $\pm$ 0.46 \\
0.2 & 31.28 $\pm$ 0.74 & 32.45 $\pm$ 1.92 & 31.28 $\pm$ 0.74 & 32.10 $\pm$ 2.72 & 16.60 $\pm$ 2.25 & 9.30 $\pm$ 0.24 \\
0.3 & 32.01 $\pm$ 1.41 & 31.67 $\pm$ 1.66 & 32.01 $\pm$ 1.41 & 31.74 $\pm$ 1.42 & 15.86 $\pm$ 2.52 & 9.40 $\pm$ 0.42 \\
0.4 & 29.96 $\pm$ 1.54 & 30.78 $\pm$ 2.32 & 29.96 $\pm$ 1.54 & 30.59 $\pm$ 2.17 & 15.55 $\pm$ 2.37 & 8.69 $\pm$ 0.74 \\
\end{tabular}\caption{
  NMI(\%) on CroppedYaleB dataset with laplace noise (mean $\pm$ std)
  \label{tab:NMI-CroppedYaleB-laplace}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 19.47 $\pm$ 0.07 & 19.86 $\pm$ 0.08 & 19.47 $\pm$ 0.07 & 19.65 $\pm$ 0.12 & 28.52 $\pm$ 0.23 & 51.79 $\pm$ 0.92 \\
0.2 & 19.68 $\pm$ 0.08 & 20.02 $\pm$ 0.06 & 19.68 $\pm$ 0.08 & 19.81 $\pm$ 0.10 & 28.20 $\pm$ 0.23 & 53.12 $\pm$ 0.90 \\
0.3 & 20.14 $\pm$ 0.09 & 20.42 $\pm$ 0.06 & 20.14 $\pm$ 0.09 & 20.21 $\pm$ 0.09 & 28.24 $\pm$ 0.18 & 56.11 $\pm$ 0.88 \\
0.4 & 20.85 $\pm$ 0.09 & 21.11 $\pm$ 0.06 & 20.85 $\pm$ 0.09 & 20.88 $\pm$ 0.07 & 28.77 $\pm$ 0.15 & 58.10 $\pm$ 1.16 \\
\end{tabular}\caption{
  RRE(\%) on CroppedYaleB dataset with uniform noise (mean $\pm$ std)
  \label{tab:RRE-CroppedYaleB-uniform}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 23.40 $\pm$ 1.08 & 24.01 $\pm$ 1.05 & 23.40 $\pm$ 1.08 & 23.21 $\pm$ 0.96 & 13.65 $\pm$ 0.48 & 8.59 $\pm$ 0.14 \\
0.2 & 22.86 $\pm$ 1.06 & 23.72 $\pm$ 0.79 & 22.86 $\pm$ 1.06 & 23.06 $\pm$ 1.20 & 12.97 $\pm$ 0.36 & 8.34 $\pm$ 0.16 \\
0.3 & 22.50 $\pm$ 0.72 & 23.70 $\pm$ 1.71 & 22.50 $\pm$ 0.72 & 22.31 $\pm$ 0.55 & 12.22 $\pm$ 0.63 & 8.22 $\pm$ 0.08 \\
0.4 & 22.80 $\pm$ 0.77 & 22.87 $\pm$ 0.92 & 22.80 $\pm$ 0.77 & 21.99 $\pm$ 1.12 & 11.27 $\pm$ 0.32 & 8.00 $\pm$ 0.21 \\
\end{tabular}\caption{
  Acc(\%) on CroppedYaleB dataset with uniform noise (mean $\pm$ std)
  \label{tab:Acc-CroppedYaleB-uniform}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 32.03 $\pm$ 0.96 & 31.99 $\pm$ 1.08 & 32.03 $\pm$ 0.96 & 31.09 $\pm$ 1.21 & 16.77 $\pm$ 0.72 & 9.05 $\pm$ 0.50 \\
0.2 & 30.56 $\pm$ 0.40 & 31.92 $\pm$ 0.72 & 30.56 $\pm$ 0.40 & 31.34 $\pm$ 0.97 & 15.67 $\pm$ 0.89 & 8.82 $\pm$ 0.24 \\
0.3 & 31.56 $\pm$ 1.74 & 31.30 $\pm$ 1.08 & 31.56 $\pm$ 1.74 & 30.77 $\pm$ 0.81 & 14.75 $\pm$ 0.84 & 9.18 $\pm$ 0.69 \\
0.4 & 31.39 $\pm$ 1.11 & 29.92 $\pm$ 1.04 & 31.39 $\pm$ 1.11 & 30.19 $\pm$ 1.59 & 12.82 $\pm$ 0.31 & 9.16 $\pm$ 0.14 \\
\end{tabular}\caption{
  NMI(\%) on CroppedYaleB dataset with uniform noise (mean $\pm$ std)
  \label{tab:NMI-CroppedYaleB-uniform}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 45.13 $\pm$ 15.14 & 44.87 $\pm$ 15.11 & 45.13 $\pm$ 15.14 & 45.26 $\pm$ 15.10 & 48.73 $\pm$ 12.92 & 59.72 $\pm$ 5.98 \\
0.2 & 80.87 $\pm$ 33.04 & 80.65 $\pm$ 33.12 & 80.87 $\pm$ 33.04 & 80.93 $\pm$ 33.03 & 82.34 $\pm$ 31.01 & 84.27 $\pm$ 22.48 \\
0.3 & 114.11 $\pm$ 47.41 & 113.97 $\pm$ 47.53 & 114.11 $\pm$ 47.41 & 114.18 $\pm$ 47.40 & 114.82 $\pm$ 45.87 & 113.24 $\pm$ 39.04 \\
0.4 & 141.66 $\pm$ 56.84 & 141.58 $\pm$ 56.97 & 141.66 $\pm$ 56.84 & 141.75 $\pm$ 56.87 & 142.30 $\pm$ 56.05 & 140.89 $\pm$ 52.29 \\
\end{tabular}\caption{
  RRE(\%) on CroppedYaleB dataset with gaussian noise (mean $\pm$ std)
  \label{tab:RRE-CroppedYaleB-gaussian}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 21.66 $\pm$ 0.76 & 22.48 $\pm$ 0.83 & 21.66 $\pm$ 0.76 & 21.36 $\pm$ 1.37 & 11.55 $\pm$ 0.82 & 8.21 $\pm$ 0.25 \\
0.2 & 19.47 $\pm$ 2.42 & 19.41 $\pm$ 2.53 & 19.47 $\pm$ 2.42 & 19.45 $\pm$ 3.13 & 10.08 $\pm$ 1.58 & 7.97 $\pm$ 0.23 \\
0.3 & 15.59 $\pm$ 4.60 & 16.05 $\pm$ 4.90 & 15.59 $\pm$ 4.60 & 15.49 $\pm$ 4.93 & 9.22 $\pm$ 1.26 & 7.84 $\pm$ 0.38 \\
0.4 & 13.05 $\pm$ 5.93 & 13.15 $\pm$ 5.77 & 13.05 $\pm$ 5.93 & 12.63 $\pm$ 5.41 & 8.81 $\pm$ 1.23 & 7.85 $\pm$ 0.32 \\
\end{tabular}\caption{
  Acc(\%) on CroppedYaleB dataset with gaussian noise (mean $\pm$ std)
  \label{tab:Acc-CroppedYaleB-gaussian}
}\end{table}
\begin{table}[H]
\begin{tabular}{c|cccccc}$\sigma$ & nmf & kl-nmf & l1-nmf & l21-nmf & cim-nmf & tanh-nmf \\\hline
0.1 & 30.13 $\pm$ 1.14 & 30.73 $\pm$ 1.89 & 30.13 $\pm$ 1.14 & 30.49 $\pm$ 2.00 & 12.96 $\pm$ 1.55 & 8.69 $\pm$ 0.37 \\
0.2 & 26.28 $\pm$ 3.37 & 26.07 $\pm$ 3.27 & 26.28 $\pm$ 3.37 & 26.50 $\pm$ 4.40 & 10.66 $\pm$ 2.58 & 8.18 $\pm$ 0.40 \\
0.3 & 19.69 $\pm$ 8.48 & 19.67 $\pm$ 7.83 & 19.69 $\pm$ 8.48 & 19.85 $\pm$ 8.24 & 9.58 $\pm$ 2.85 & 7.61 $\pm$ 1.03 \\
0.4 & 15.37 $\pm$ 9.62 & 15.21 $\pm$ 8.78 & 15.37 $\pm$ 9.62 & 14.38 $\pm$ 8.90 & 9.32 $\pm$ 2.55 & 7.48 $\pm$ 0.70 \\
\end{tabular}\caption{
  NMI(\%) on CroppedYaleB dataset with gaussian noise (mean $\pm$ std)
  \label{tab:NMI-CroppedYaleB-gaussian}
}\end{table}

\end{document}
